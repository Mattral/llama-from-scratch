{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "\n",
    "lines = open('./input.txt', 'r').read()\n",
    "\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "print('vocab size:', len(vocab))\n",
    "decode(encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s\\nFrom me ', '\\nFrom me r'),\n",
       " ('nnot be\\nMi', 'not be\\nMin'),\n",
       " ('s and me t', ' and me th'),\n",
       " ('d am I the', ' am I then'),\n",
       " ('.\\nTell Bol', '\\nTell Boli'),\n",
       " (' Citizen:\\n', 'Citizen:\\nW'),\n",
       " ('ht.\\nLords,', 't.\\nLords, '),\n",
       " ('nd for our', 'd for our '),\n",
       " ('ns?\\nTheref', 's?\\nTherefo'),\n",
       " (' men are a', 'men are at'),\n",
       " ('l.\\n\\nJULIET', '.\\n\\nJULIET:'),\n",
       " ('a travel t', ' travel th'),\n",
       " (\"thy day's \", \"hy day's w\"),\n",
       " ('uld have a', 'ld have as'),\n",
       " (', but then', ' but then '),\n",
       " ('h if I hav', ' if I have'),\n",
       " ('How now! w', 'ow now! wh'),\n",
       " ('o thy bark', ' thy bark:'),\n",
       " (\"earn'd eve\", \"arn'd even\"),\n",
       " ('mine age,\\n', 'ine age,\\nA'),\n",
       " ('eed; for h', 'ed; for he'),\n",
       " ('E:\\nNay, co', ':\\nNay, com'),\n",
       " ('He is, my ', 'e is, my l'),\n",
       " ('s thou art', ' thou art '),\n",
       " ('y comes ba', ' comes bac'),\n",
       " ('n a pile\\nO', ' a pile\\nOf'),\n",
       " (' be that p', 'be that pr'),\n",
       " ('and waitin', 'nd waiting'),\n",
       " ('ng that:\\nS', 'g that:\\nSh'),\n",
       " ('\\nBUCKINGHA', 'BUCKINGHAM'),\n",
       " ('brace him;', 'race him; '),\n",
       " ('ll men, I ', 'l men, I h')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"d_model\": 100,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    'batch_size': 32,\n",
    "    'context_window': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "    \n",
    "    batch_data = train\n",
    "    if split == 'val':\n",
    "        batch_data = val\n",
    "    \n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y\n",
    "\n",
    "xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "\n",
    "[(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Swish-Gated Linear Unit\n",
    "    https://arxiv.org/pdf/2002.05202v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.linear_gate = nn.Linear(size, size)\n",
    "        self.linear = nn.Linear(size, size)\n",
    "        self.beta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, x): \n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = swish_gate * self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n",
    "        \"\"\"\n",
    "            Root Mean Square Layer Normalization\n",
    "        :param d: model size\n",
    "        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n",
    "        :param eps:  epsilon value, default 1e-8\n",
    "        :param bias: whether use bias term for RMSNorm, disabled by\n",
    "            default because RMSNorm doesn't enforce re-centering invariance.\n",
    "\n",
    "        https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py\n",
    "        \"\"\"\n",
    "        super(RMSNorm, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.bias = bias\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.register_parameter(\"scale\", self.scale)\n",
    "\n",
    "        if self.bias:\n",
    "            self.offset = nn.Parameter(torch.zeros(d))\n",
    "            self.register_parameter(\"offset\", self.offset)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.p < 0. or self.p > 1.:\n",
    "            norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = self.d\n",
    "        else:\n",
    "            partial_size = int(self.d * self.p)\n",
    "            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n",
    "\n",
    "            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = partial_size\n",
    "\n",
    "        rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        x_normed = x / (rms_x + self.eps)\n",
    "\n",
    "        if self.bias:\n",
    "            return self.scale * x_normed + self.offset\n",
    "\n",
    "        return self.scale * x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3992, 1.4248, 1.1312, 1.0985, 0.9464, 0.8868, 0.8526, 0.7926, 0.7011,\n",
       "        0.6074], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LlamaAttentionLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.w_q = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.w_k = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.w_v = nn.Linear(config['d_model'], config['d_model'])\n",
    "\n",
    "        self.multihead = nn.MultiheadAttention(config['d_model'], config['n_heads'], dropout=0.1, batch_first=True)\n",
    "        self.rms = RMSNorm(config['d_model'])\n",
    "\n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        b,m,d = x.shape\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        activations, attn_weights = self.multihead(q,k,v, attn_mask=torch.tril(torch.ones(m,m)), is_causal=True)\n",
    "        if return_attn_weights:\n",
    "            return self.rms(x + activations), attn_weights\n",
    "        return self.rms(x + activations) # TOOD: this is post-norm, try pre-norm\n",
    "\n",
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 128,\n",
    "}\n",
    "m = LlamaAttentionLayer(config)\n",
    "batch = torch.randn(1, 10, config['d_model'])\n",
    "activations, attn_weights = m(batch, return_attn_weights=True)\n",
    "plt.imshow(attn_weights.detach().numpy()[0])\n",
    "\n",
    "attn_weights.squeeze(0).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.attention = LlamaAttentionLayer(config)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "        )\n",
    "        self.rms = RMSNorm(config['d_model'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        out = self.rms(attended + self.feedforward(attended))\n",
    "        return out\n",
    "    \n",
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 128,\n",
    "}\n",
    "m = LlamaBlock(config)\n",
    "batch = torch.randn(1, 10, config['d_model'])\n",
    "m(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n params: 233665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"\\nE$uf'voZYP3PvNLfAnUL\",\n",
       " '\\nPQu\\ndwA3IZUpnbaLBpOU',\n",
       " '\\nz-xFepEKLNTF-zbq;.e;',\n",
       " '\\nU&WUpbd pYIU:VjBIIQx',\n",
       " \"\\ne?;zpC$I?skae'QhQkX$\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        self.positional_embeddings = nn.Embedding(config['context_window'], config['d_model'])\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])])\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embeddings(idx)\n",
    "        x = x + self.positional_embeddings(torch.arange(x.shape[1])).unsqueeze(0)\n",
    "        x = self.llama_blocks(x)\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        \n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens=20):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # call the model\n",
    "            logits = self(idx[:, -config['context_window']:])\n",
    "            last_time_step_logits = logits[\n",
    "                :, -1, :\n",
    "            ]  # all the batches (1), last time step, all the logits\n",
    "            p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities\n",
    "            idx_next = torch.multinomial(\n",
    "                p, num_samples=1\n",
    "            )  # sample from the distribution to get the next token\n",
    "            idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence\n",
    "        return idx\n",
    "            \n",
    "config = {\n",
    "    \"n_heads\": 1,\n",
    "    \"d_model\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"n_layers\": 1,\n",
    "    \"context_window\": 16,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"epochs\": 1000,\n",
    "    \"log_interval\": 50,\n",
    "}\n",
    "m = Llama(config)\n",
    "xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "m(xs, targets=ys)\n",
    "\n",
    "print(f\"n params: {sum([p.numel() for p in m.parameters() if p.requires_grad])}\")\n",
    "\n",
    "[decode(x) for x in m.generate(torch.zeros(5, 1).long()).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | val loss 4.170 | Time 0.069 | ETA in seconds 13.823\n",
      "[\"\\n3E3 3eT'DQ;nz$d-iCXc\", \"\\nWGgpf'IjrHOmaGtwln .\", '\\nwHrVAXyRj?;hur?.$Rep', '\\nw\\n\\n-uG.x:OVBhd3ZN vu', '\\n?DHs.LvCMqKug:3KrjfI']\n",
      "Epoch 50 | val loss 2.765 | Time 2.263 | ETA in seconds 450.426\n",
      "[\"\\n.ToZPBT?sBhDt'!NxELq\", '\\n.nKQExZw3,ZUQHlw;3B,', \"\\nmE3uj'jLE&WAlmnGejuE\", \"\\nXoGcWbg.OOAUP'yK?fIP\", \"\\n.ZuZN3v-Yn',ChVmsMau\"]\n",
      "Epoch 100 | val loss 1.298 | Time 2.246 | ETA in seconds 444.646\n",
      "[\"\\noBjRuk?st'WhQCO:kq&n\", '\\n3lstES!Owm$rOU,mMpum', \"\\nR,Y;bVoJlrYbfIEH'!Jd\", '\\n-MrEmHCuk-P.lYHh YL:', '\\nB,S:SlxtqGi$hMr.bF:U']\n",
      "Epoch 150 | val loss 0.271 | Time 2.273 | ETA in seconds 447.827\n",
      "['\\neGsCm\\n!p;Ez3;LjEBwqW', '\\nA$-ZqpF-c.wJTDX3\\nXhd', '\\nCD:CuoZcPHXPn-\\ngM;AU', '\\nqsDwRZRv$YvAcKJ.ODc.', '\\nH$ptAaVr-&ZsxShcq$CA']\n",
      "Epoch 200 | val loss 0.181 | Time 2.245 | ETA in seconds 439.993\n",
      "['\\nWZXuDIwBI LOou?NWfmk', '\\nucNYwIhXGLtC.MQsvnWP', '\\nu,jQttCjQYPb sbL&aFZ', '\\nI-ixaQ?a&a L zl?MaNN', \"\\nnnIXjF$TJzkArv-'ufiU\"]\n",
      "Epoch 250 | val loss 0.177 | Time 2.235 | ETA in seconds 435.800\n",
      "[\"\\nxKLG?,'shbVIspAi\\nLiW\", '\\nLbsN!;Y.leRs&kVZp:R-', '\\n,JDnSJn-Rk?&ag or\\neN', '\\nCsgkbulNT axq\\nvsdlmd', '\\nX?yXPAfBi$UKARj?-3Mu']\n",
      "Epoch 300 | val loss 0.170 | Time 2.241 | ETA in seconds 434.761\n",
      "['\\n:C!FW?IIDUjlOFd;Hdqb', '\\nYHRMeo$kyYFzcNhzp:Cr', '\\nvgoTLBp3;vgCxCp:xgX!', '\\nJSXojJoKqhN-mWC&CkHs', '\\nD;n,ge&RkrUXctOvC3g-']\n",
      "Epoch 350 | val loss 0.175 | Time 2.298 | ETA in seconds 443.595\n",
      "[\"\\nyPwNcoYUFB\\n m':Fi'.j\", '\\npj;HfWPn&$;&cLFloqJU', '\\nmkgf3GB&!&cmXfZzL\\nvR', '\\nzx!tmu;Lv.CMBVk d!;V', \"\\nFHYbu,RTCNwdB'mIm!AE\"]\n",
      "Epoch 400 | val loss 0.164 | Time 2.307 | ETA in seconds 442.965\n",
      "['\\nHrGJ?uqSK-tiBk;y3;m\\n', '\\nXpKv\\ns,J:C kJXc.Nm&\\n', '\\nwSC3Ar?XVermnwhAwu!g', '\\nmAxUtjgjQJqeRl;OvpUV', \"\\nWNB!M!ITTqbNg3HCiAm'\"]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     30\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mlog_interval\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/optim/adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    341\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"n_layers\": 4,\n",
    "    \"context_window\": 16,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"epochs\": 10000,\n",
    "    \"log_interval\": 50,\n",
    "}\n",
    "model = Llama(config)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 300, eta_min=1e-5)\n",
    "\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "for epoch in range(config['epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "    logits, loss = model(xs, targets=ys)\n",
    "    loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch % config['log_interval'] == 0:\n",
    "        batch_time = time.time() - start_time\n",
    "        x = evaluate_loss(model)\n",
    "        losses += [x]\n",
    "        print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        print([decode(x) for x in m.generate(torch.zeros(5, 1).long()).tolist()])\n",
    "\n",
    "pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KKKKKKKKKKK', 'TTTTTTTTTTT', 'ggggggggggg', 'HHHHHHGHHHH', 'IIIIIIIIIII']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model, idx, max_length=10):\n",
    "    x = idx\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            sequence = x[:, -model.config['context_window']:]\n",
    "            logits = model(sequence)[:,-1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            pred = torch.multinomial(probs, 1, replacement=True)\n",
    "            x = torch.cat([x, pred], dim=-1)\n",
    "        return x\n",
    "\n",
    "[decode(x) for x in generate(model, torch.randint(0, len(vocab), (5, 1)).long()).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
