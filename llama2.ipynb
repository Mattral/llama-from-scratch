{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "\n",
    "lines = open('./input.txt', 'r').read()\n",
    "\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "print(vocab)\n",
    "print('vocab size:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0], dtype=torch.int8)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\nPOMPEY:\\nV', 'POMPEY:\\nVe'),\n",
       " ('\\n\\nQUEEN MA', '\\nQUEEN MAR'),\n",
       " ('hollow eye', 'ollow eyes'),\n",
       " ('hanging.\\n\\n', 'anging.\\n\\nP'),\n",
       " (' is that g', 'is that go'),\n",
       " (' he marrie', 'he married'),\n",
       " ('s one yond', ' one yonde'),\n",
       " ('spositions', 'positions '),\n",
       " ('fter holid', 'ter holida'),\n",
       " (\"nd where's\", \"d where's \"),\n",
       " ('ech you, t', 'ch you, ta'),\n",
       " ('Clown:\\nCom', 'lown:\\nCome'),\n",
       " ('n his cour', ' his cours'),\n",
       " (', defend m', ' defend me'),\n",
       " ('IUS:\\nThe s', 'US:\\nThe se'),\n",
       " ('ARD IV:\\nBr', 'RD IV:\\nBro'),\n",
       " ('rey\\nWere f', 'ey\\nWere fa'),\n",
       " ('progress t', 'rogress to'),\n",
       " ('its on the', 'ts on the '),\n",
       " ('\\nTo think ', 'To think i'),\n",
       " (' mistake.\\n', 'mistake.\\n\\n'),\n",
       " ('s raising;', ' raising;\\n'),\n",
       " (' hear\\nIs t', 'hear\\nIs th'),\n",
       " ('\\n\\nFLORIZEL', '\\nFLORIZEL:'),\n",
       " ('\\nOf my boy', \"Of my boy'\"),\n",
       " ('nt.\\nThe wr', 't.\\nThe wre'),\n",
       " ('vault,\\nTha', 'ault,\\nThat'),\n",
       " ('RLAND:\\nBe ', 'LAND:\\nBe i'),\n",
       " ('ct,\\nUnder ', 't,\\nUnder w'),\n",
       " ('ure to die', 're to die.'),\n",
       " ('ce I will ', 'e I will r'),\n",
       " (' like shad', 'like shado')]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"d_model\": 100,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    'batch_size': 32,\n",
    "    'context_window': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "    \n",
    "    batch_data = train\n",
    "    if split == 'val':\n",
    "        batch_data = val\n",
    "    \n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y\n",
    "\n",
    "xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "\n",
    "[(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Swish-Gated Linear Unit\n",
    "    https://arxiv.org/pdf/2002.05202v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.linear_gate = nn.Linear(size, size)\n",
    "        self.linear = nn.Linear(size, size)\n",
    "        self.beta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, x): \n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = swish_gate * self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n",
    "        \"\"\"\n",
    "            Root Mean Square Layer Normalization\n",
    "        :param d: model size\n",
    "        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n",
    "        :param eps:  epsilon value, default 1e-8\n",
    "        :param bias: whether use bias term for RMSNorm, disabled by\n",
    "            default because RMSNorm doesn't enforce re-centering invariance.\n",
    "\n",
    "        https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py\n",
    "        \"\"\"\n",
    "        super(RMSNorm, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.bias = bias\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.register_parameter(\"scale\", self.scale)\n",
    "\n",
    "        if self.bias:\n",
    "            self.offset = nn.Parameter(torch.zeros(d))\n",
    "            self.register_parameter(\"offset\", self.offset)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.p < 0. or self.p > 1.:\n",
    "            norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = self.d\n",
    "        else:\n",
    "            partial_size = int(self.d * self.p)\n",
    "            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n",
    "\n",
    "            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = partial_size\n",
    "\n",
    "        rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        x_normed = x / (rms_x + self.eps)\n",
    "\n",
    "        if self.bias:\n",
    "            return self.scale * x_normed + self.offset\n",
    "\n",
    "        return self.scale * x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4795, 1.2524, 1.1793, 1.1886, 0.9937, 0.9293, 0.8054, 0.7651, 0.6825,\n",
       "        0.6113], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LlamaAttentionLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.w_q = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.w_k = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.w_v = nn.Linear(config['d_model'], config['d_model'])\n",
    "\n",
    "        self.multihead = nn.MultiheadAttention(config['d_model'], config['n_heads'], dropout=0.1, batch_first=True)\n",
    "        self.rms = RMSNorm(config['d_model'])\n",
    "\n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        b,m,d = x.shape\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        activations, attn_weights = self.multihead(q,k,v, attn_mask=torch.tril(torch.ones(m,m)), is_causal=True)\n",
    "        if return_attn_weights:\n",
    "            return self.rms(x + activations), attn_weights\n",
    "        return self.rms(x + activations) # TOOD: this is post-norm, try pre-norm\n",
    "\n",
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 128,\n",
    "}\n",
    "m = LlamaAttentionLayer(config)\n",
    "batch = torch.randn(1, 10, config['d_model'])\n",
    "activations, attn_weights = m(batch, return_attn_weights=True)\n",
    "plt.imshow(attn_weights.detach().numpy()[0])\n",
    "\n",
    "attn_weights.squeeze(0).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.attention = LlamaAttentionLayer(config)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "        )\n",
    "        self.rms = RMSNorm(config['d_model'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        out = self.rms(attended + self.feedforward(attended))\n",
    "        return out\n",
    "    \n",
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 128,\n",
    "}\n",
    "m = LlamaBlock(config)\n",
    "batch = torch.randn(1, 10, config['d_model'])\n",
    "m(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n params: 233665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nuw\\nitObmt?cIrSSh;U3A',\n",
       " '\\n?pWozP-IJQe fAeBb.Wk',\n",
       " \"\\nzlCegw'Gm-P.tfAAjQ-V\",\n",
       " '\\nTtV\\nUEp,NRGtLvDVs? n',\n",
       " '\\n?zbMigBju$&VcHOvi QS']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        self.positional_embeddings = nn.Embedding(config['context_window'], config['d_model'])\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])])\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embeddings(idx)\n",
    "        x = x + self.positional_embeddings(torch.arange(x.shape[1]))\n",
    "        x = self.llama_blocks(x)\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        \n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_length=20):\n",
    "        x = idx\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                sequence = x[:, -self.config['context_window']:]\n",
    "                logits = self(sequence)[:,-1]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                pred = torch.multinomial(probs, 1, replacement=True)\n",
    "                x = torch.cat([x, pred], dim=1)\n",
    "            return x\n",
    "            \n",
    "config = {\n",
    "    \"n_heads\": 1,\n",
    "    \"d_model\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"n_layers\": 1,\n",
    "    \"context_window\": 16,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"epochs\": 1000,\n",
    "    \"log_interval\": 50,\n",
    "}\n",
    "m = Llama(config)\n",
    "xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "m(xs, targets=ys)\n",
    "\n",
    "print(f\"n params: {sum([p.numel() for p in m.parameters() if p.requires_grad])}\")\n",
    "\n",
    "[decode(x) for x in m.generate(torch.zeros(5, 1).long()).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | val loss 4.169 | Time 0.043 | ETA in seconds 8.618\n",
      "['\\nZGxiVGHU?LSLLNJLx YM', \"\\n':FERPRU?'NFm,,x'Rvh\", '\\nbmo\\nhTliERq,!RrzIaW!', \"\\nyeuhdSJA?ajC;g''obhU\", '\\nSSHAfcY,PCKr,-lMIwdf']\n",
      "Epoch 50 | val loss 2.661 | Time 2.262 | ETA in seconds 450.152\n",
      "[\"\\n.Dt,gR3VXh'lTXvFB:Z,\", '\\n,fmfa-,dTbEJ\\n-h3RRuk', '\\nblsoMPrC&QVdKxLU?WGA', '\\nB.Lj;WoFIquzz;EDIQib', \"\\nQ;MmCxh$-:'-P'qYRaYD\"]\n",
      "Epoch 100 | val loss 0.525 | Time 2.015 | ETA in seconds 399.045\n",
      "['\\nsR!jwnbUUiZrJ,SLB3ag', '\\n,?:EFAcpUkVj$pWERO3T', '\\n&cIIJzMyglL!JT,XosQ!', '\\nDsFYwpvSQFcXa;OcIxC ', \"\\nm-awr.yP\\naYqk'fMzbUq\"]\n",
      "Epoch 150 | val loss 0.197 | Time 1.954 | ETA in seconds 384.941\n",
      "['\\nbPouoVZUhbT iv!ZUvWy', '\\n&Ok3gVsPUF&AeZXCW.hN', '\\nGttbneyfC&fKpdnoqKjj', '\\n:qYwM?yaloGAWZTus,Al', '\\nTYJxcIn-M&&bnkbbK,!L']\n",
      "Epoch 200 | val loss 0.179 | Time 1.965 | ETA in seconds 385.044\n",
      "['\\niP$UN-HTXcaa\\nZbIq3xh', \"\\nEmJjL:cpv&IQl'MsBR.s\", '\\n3exKI!cmvgeK&n?iPcOH', '\\n-.,P$Uuwo!nWkiOMjvfO', '\\nXuqNLIrQruILRhI.-OMd']\n",
      "Epoch 250 | val loss 0.171 | Time 1.960 | ETA in seconds 382.250\n",
      "['\\nRQYCmM-VubDbW&azQOGY', \"\\ng'de;MSRBuUE.i-Ra-jD\", \"\\n'\\nTdziCSdjBLmZTX.NQu\", '\\nR&MSXw3WQFD;Nls?vBOZ', '\\noDr3ZB\\n;DQyJZdg3MIHC']\n",
      "Epoch 300 | val loss 0.166 | Time 1.963 | ETA in seconds 380.736\n",
      "['\\nZ?: TpXT-XhyTF?A$?gX', '\\n el.quPWyUtElQZL\\nhbZ', \"\\nuPyXbJrV'Dn xVYY$Zkk\", \"\\nlAM'SKg\\nqYtTLUV$qpF&\", '\\nufuyWuoS:dFaZJkGzStB']\n",
      "Epoch 350 | val loss 0.167 | Time 1.959 | ETA in seconds 378.024\n",
      "['\\ns:aloi-subjzeerDOWE3', '\\nSG&O\\nj-qniZsZ\\nT!fVZx', '\\nLbmJKKv GXFl3RiQMiGR', \"\\niu Tn'-cV?Y'YZRpYI$u\", '\\nwTjO!-Tu$k;$D,nYSYh3']\n",
      "Epoch 400 | val loss 0.164 | Time 1.947 | ETA in seconds 373.760\n",
      "['\\nCt,gZHPmxxdw--QiXKzn', \"\\nlf'BMeQont detHsnZt3\", '\\n.Yf;R&w,ugjyb$F\\nyrvn', '\\nGG!3XkbZUX wjSSb&v$s', '\\nR ddi!YnyhJm o ;$&:x']\n",
      "Epoch 450 | val loss 0.167 | Time 1.974 | ETA in seconds 377.073\n",
      "['\\nq,gOtngzzrhYd XPlaR.', '\\ncDsRVD!N!wukIWi\\nx\\nJn', \"\\n:--cdI'n-V .p;,T-o$I\", '\\nhYwMf$UuEBJiIA,YRD;N', '\\n-HAi!&yuV JCe\\ngxO:$,']\n",
      "Epoch 500 | val loss 0.159 | Time 1.951 | ETA in seconds 370.750\n",
      "[\"\\nZ,IBdiS.'$z\\n gr sIoG\", '\\nMrn&rhP!$uLu;z3,T.En', '\\nvG;N&vatsk:$qo\\nk3,ls', '\\nSELYVjayfGmgBQ;UlVRT', '\\nMfvY3$b.eRqlkbNBLBCY']\n",
      "Epoch 550 | val loss 0.185 | Time 1.949 | ETA in seconds 368.446\n",
      "[\"\\nKJ-c-iOfk'WGTvwoLc?h\", '\\ngllE?Z$aYUHVsLTKLdsF', '\\n yQ$cJmStn br!U&E;;!', \"\\nZnWWlYf!XYTU&i?'wgwk\", '\\n3&gXo\\nYWtRX uI;;YbJ?']\n",
      "Epoch 600 | val loss 0.164 | Time 1.957 | ETA in seconds 367.943\n",
      "['\\n$KMeyutICv&MiJfYl,WJ', '\\nEaKUlXMH,o-ZVarqJbXU', '\\ndxTkgdYTuxukX$QUjqHg', '\\nR&ZVIowOgKSJs!wc.zS!', \"\\nUKux&HC;x;.sO;w3eZI'\"]\n",
      "Epoch 650 | val loss 0.164 | Time 1.970 | ETA in seconds 368.353\n",
      "[\"\\nNEwycJu$JZVki'RvrleU\", \"\\n3wbC3S'yGRFw.nWdxJt\\n\", '\\nlFjNIbNBIF XbbdBjqZF', '\\nyhlTpmXXWlJ-mz$qo?K:', '\\nwwXUKpwr!P$CnyhkesnZ']\n",
      "Epoch 700 | val loss 0.156 | Time 2.008 | ETA in seconds 373.417\n",
      "['\\nq3LpFoDZ!-S?GYwGPI:u', '\\nMoq!R:-hBB?xWDoBE-vv', \"\\n&DmGWWRrbMr'!yCtvcCO\", \"\\n' 'rjzANWXgT-yEWrU?j\", '\\ni:YUFu;fyxbmZyFQ3WMW']\n",
      "Epoch 750 | val loss 0.155 | Time 2.027 | ETA in seconds 375.072\n",
      "['\\nnDcqbXcOdUE&c i:kIe\\n', '\\nUEkk.wOZHEnGgTZ$\\nNO3', '\\nPCct3CesLDW.CR!nc.MP', \"\\nG!sqBpCKOM 3MbVmKSS'\", '\\nAdtvPzDVC:NdmayNsES&']\n",
      "Epoch 800 | val loss 0.149 | Time 1.956 | ETA in seconds 359.873\n",
      "[\"\\nr:P:kSU'CoKWMR'n ucx\", '\\nKmg,!\\nhU.BM-IKbKDuO?', '\\nutOqWvnxLPuEZIa!Rhmv', '\\nENBV3Li.AoQnyc.K3cpJ', \"\\nXRvSrMjX;SQaluMrCE'd\"]\n",
      "Epoch 850 | val loss 0.142 | Time 1.966 | ETA in seconds 359.770\n",
      "[\"\\nu,d&qKx:JHoDfbLV'T,e\", '\\ntj:yS:Xe!xDfUAtwVadD', '\\nmeLpuq&HpIDo tpFI?ZI', '\\nP?MOUEAgdOiH pro,iFx', \"\\nzRwSctWOBi?t'hfWgolz\"]\n",
      "Epoch 900 | val loss 0.156 | Time 1.985 | ETA in seconds 361.286\n",
      "[\"\\nhnnsPOArm-sJJAiB'SZj\", \"\\nXAwLdgaaCyc'!dWue!VW\", '\\nVcI-tjJ,xuJtuazRLPkf', \"\\npKQ:cdTy\\n'H$M&OxthHI\", '\\nfOdb-mBgUGBq qDht!jP']\n",
      "Epoch 950 | val loss 0.145 | Time 1.951 | ETA in seconds 353.163\n",
      "['\\n;xXZ?seVRFjJ ?xifdJQ', '\\niIUZEFdLdTxuO\\nB,.ZgB', '\\n!P3BKrzElusgf!Im&!Hp', '\\nNi,VtJFSSpq,GNzJ&Xz.', \"\\nqgwRdjwdDjoAB&buiq':\"]\n",
      "Epoch 1000 | val loss 0.154 | Time 1.941 | ETA in seconds 349.377\n",
      "[\"\\n: !qrNdFrX&HaQl'YMev\", '\\ngA!ERLZisUiLu:u?\\nbE;', '\\nbZT$EEUsEJgdoLUJCxHe', '\\nj$Fksg3zHPhACuAKcyZs', '\\nsLd;3ldEYJSpxNZ!r?&z']\n",
      "Epoch 1050 | val loss 0.157 | Time 1.951 | ETA in seconds 349.168\n",
      "['\\nnOkPu3;USenTcOI\\nT-tk', '\\nW\\n FecfGg-3ggq;t.WSs', '\\nAkI-LH-Pdq;QUFzEkqS,', \"\\ngiiYWcV:;,LIv.'.fQp.\", '\\nK\\nM$EDY3Q!yVH&;n3o\\nc']\n",
      "Epoch 1100 | val loss 0.156 | Time 1.949 | ETA in seconds 347.001\n",
      "[\"\\nUQ3WIP;E3jtjU'WQyvM,\", '\\njRa ,x!HswG-GgmGGPNb', '\\nsSV owgd&UfKNsrTlaj,', '\\nuxjR&j GS?su!UAkROvo', '\\nKCnzfQBAsmG!jTFOc;iI']\n",
      "Epoch 1150 | val loss 0.153 | Time 1.931 | ETA in seconds 341.765\n",
      "[\"\\nUsr'qD,Vn'AXMNoZsv. \", \"\\nX;FJUHtWZMlMVxW!?og'\", \"\\n;ISN!'loOzdnyj$AiDQq\", '\\nhy$3,-?Z,kdWMP,r-DfN', '\\n:?qcbDgpit N?ToGWMsf']\n",
      "Epoch 1200 | val loss 0.154 | Time 1.945 | ETA in seconds 342.285\n",
      "['\\n;kzCCkJneENUgXZ3VApe', '\\nvo$cd!d.esDU.jWTSZin', \"\\nA-kbn;eXP; WMw'XbVom\", \"\\n?'l'rOmx.Ft$:y&X;!HV\", \"\\nPsEx\\ni!RZjZ.SADCa'LR\"]\n",
      "Epoch 1250 | val loss 0.172 | Time 1.940 | ETA in seconds 339.568\n",
      "['\\n!AyWdRq!b,dAX?fGHn!N', '\\nITY; wk,,WXLu:tkSoOB', \"\\nccrwenaFac'lOv?CHm-v\", \"\\ndQW;rBWfKGZcb'b-aJ;3\", '\\ndbtZJHeDbBpZYzuxFf3b']\n",
      "Epoch 1300 | val loss 0.151 | Time 1.987 | ETA in seconds 345.754\n",
      "[\"\\npFdnBcorMifFqlfz;'WU\", '\\nFw;cpuxksIgP;;idpF!:', '\\nU MWAw?vitCKdD!;yDtA', \"\\nBsq OB$owQr''VgUoBPv\", '\\nt!GoxB yr;&.N, ,aa?m']\n",
      "Epoch 1350 | val loss 0.148 | Time 1.979 | ETA in seconds 342.378\n",
      "['\\n;LchUU&NOtqxkxpppKgr', '\\nv;orWiv,fb;xYQ!$ZQr ', '\\n,:E:s-YZSqyfDZSmP LU', \"\\nHYCUWnFdQe?wA;UJ,Cv'\", '\\ntEDmqJKvrR?TncagiYul']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m xs, ys \u001b[39m=\u001b[39m get_batches(dataset, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39mcontext_window\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m logits, loss \u001b[39m=\u001b[39m model(xs, targets\u001b[39m=\u001b[39mys)\n\u001b[0;32m---> 26\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     28\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[1;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"n_layers\": 4,\n",
    "    \"context_window\": 16,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"epochs\": 10000,\n",
    "    \"log_interval\": 50,\n",
    "}\n",
    "model = Llama(config)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 300, eta_min=1e-5)\n",
    "\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "for epoch in range(config['epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "    logits, loss = model(xs, targets=ys)\n",
    "    loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch % config['log_interval'] == 0:\n",
    "        batch_time = time.time() - start_time\n",
    "        x = evaluate_loss(model)\n",
    "        losses += [x]\n",
    "        print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        print([decode(x) for x in m.generate(torch.zeros(5, 1).long()).tolist()])\n",
    "\n",
    "pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KKKKKKKKKKK', 'TTTTTTTTTTT', 'ggggggggggg', 'HHHHHHGHHHH', 'IIIIIIIIIII']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model, idx, max_length=10):\n",
    "    x = idx\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            sequence = x[:, -model.config['context_window']:]\n",
    "            logits = model(sequence)[:,-1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            pred = torch.multinomial(probs, 1, replacement=True)\n",
    "            x = torch.cat([x, pred], dim=-1)\n",
    "        return x\n",
    "\n",
    "[decode(x) for x in generate(model, torch.randint(0, len(vocab), (5, 1)).long()).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
