{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "\n",
    "lines = open('./input.txt', 'r').read()\n",
    "\n",
    "vocab = list(set(lines))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "print('vocab size:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23, 64, 21,  ..., 42, 31, 18], dtype=torch.int8)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run training with minibatches of size 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"d_model\": 100,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    'batch_size': 32,\n",
    "    'context_window': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train, val, test = data[:int(.8 * len(data))], data[int(.8 * len(data)): int(.9 * len(data))], data[int(.9 * len(data)):]\n",
    "    \n",
    "    if split == 'train':\n",
    "        data = train\n",
    "    elif split == 'val':\n",
    "        data = val\n",
    "    \n",
    "    xs = torch.zeros(batch_size, context_window, dtype=torch.long)\n",
    "    ys = torch.zeros(batch_size, context_window, dtype=torch.long)\n",
    "\n",
    "    # pick random starting points\n",
    "    starts = torch.randint(0, len(data) - context_window - 1, (batch_size,))\n",
    "    for item, start in enumerate(starts):\n",
    "        \n",
    "        xs[item] += data[start:start+context_window]\n",
    "        ys[item] += data[start+1:start+context_window+1]\n",
    "    return xs, ys\n",
    "    \n",
    "xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 128])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskedRotarySelfAttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary embeddings.\n",
    "\n",
    "    Input: (BATCH_SIZE x CONTEXT_WINDOW)\n",
    "    Output: (BATCH_SIZE x CONTEXT_WINDOW x EMBEDDING_DIM)\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if config['d_model'] % 2 != 0:\n",
    "            raise ValueError(\"d_model must be divisible by 2\")\n",
    "        self.w_q = torch.randn(config['d_model'], config['d_model'])\n",
    "        self.w_k = torch.randn(config['d_model'], config['d_model'])\n",
    "        self.w_v = torch.randn(config['d_model'], config['d_model'])\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(config['context_window'], config['context_window']))) # mask\n",
    "        \n",
    "        self.R = self.get_rotary_matrix() # [d_model x d_model x context_window]\n",
    "\n",
    "    def get_rotary_matrix(self):\n",
    "        context_window = self.config['context_window']\n",
    "        d = self.config['d_model']\n",
    "        R = torch.zeros((d, d, context_window))\n",
    "        for m in range(context_window):\n",
    "            for i in range(d//2):\n",
    "                theta = 10000. ** (-2.*(i - 1) / d)\n",
    "                R[2*i,2*i, m] = np.cos(m * theta)\n",
    "                R[2*i,2*i+1, m] = - np.sin(m * theta)\n",
    "                R[2*i+1,2*i, m] = np.sin(m * theta)\n",
    "                R[2*i+1,2*i+1, m] = np.cos(m * theta)\n",
    "        return R\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [BATCH_SIZE x CONTEXT_WINDOW x EMBEDDING_DIM]\n",
    "        out: [BATCH_SIZE x CONTEXT_WINDOW x EMBEDDING_DIM]\n",
    "        \"\"\"\n",
    "        b = self.config['batch_size']\n",
    "        d = self.config['d_model']\n",
    "        m = self.config['context_window']\n",
    "\n",
    "        q_rotated_query_weight = self.w_q @ self.R # [d x d x m]\n",
    "        q = q_rotated_query_weight.view(m, d, d) @ x.view(m, d, b)\n",
    "        q = q.view(b, m, d)\n",
    "\n",
    "        k_rotated_query_weight = self.w_k @ self.R # [d x d x m]\n",
    "        k = k_rotated_query_weight.view(m, d, d) @ x.view(m, d, b)\n",
    "        k = k.view(b, m, d)\n",
    "        \n",
    "        B = (q @ k.transpose(1,2)) / np.sqrt(self.config['d_model'])\n",
    "        mask = B.masked_fill(self.tril[:m, :m] == 0, float(\"-inf\"))\n",
    "        a = F.softmax(mask, dim=-1) # attention\n",
    "        v = x @ self.w_v \n",
    "        out = a @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 3,\n",
    "    \"context_window\": 10,\n",
    "    \"d_model\": 128,\n",
    "}\n",
    "r = RotarySelfAttention(config)\n",
    "batch = torch.randn((config['batch_size'], config['context_window'], config['d_model']))\n",
    "\n",
    "r(batch).shape\n",
    "\n",
    "# ==== check ====\n",
    "# s = batch[0,:,:]\n",
    "# (r.w_q @ r.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 128])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskedRotaryNultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            MaskedRotarySelfAttentionHead(config) for _ in range(config[\"n_heads\"])\n",
    "        ])\n",
    "        self.linear = nn.Linear(config[\"n_heads\"] * config[\"d_model\"], config[\"d_model\"])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.linear(out))\n",
    "        return out\n",
    "\n",
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"context_window\": 16,\n",
    "}\n",
    "\n",
    "batch = torch.randn((config['batch_size'], config['context_window'], config['d_model']))\n",
    "m = MaskedRotaryNultiHeadedAttention(config)\n",
    "\n",
    "m(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Swish-Gated Linear Unit\n",
    "    https://arxiv.org/pdf/2002.05202v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.linear = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.beta = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * torch.sigmoid(self.beta(x)) + (1 - torch.sigmoid(self.beta(x))) * self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 512,\n",
    "    \"batch_size\": 3,\n",
    "    \"context_window\": 10,\n",
    "}\n",
    "\n",
    "class LlamaLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.prenorm = nn.LayerNorm(config['d_model'])\n",
    "        self.multihead = MaskedRotaryNultiHeadedAttention(config)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config),\n",
    "        )\n",
    "        self.postnorm = nn.LayerNorm(config['d_model'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mid = x + self.multihead(self.prenorm(x))\n",
    "        out = mid + self.ffn(mid)\n",
    "\n",
    "        return out\n",
    "\n",
    "batch = torch.randn((config['batch_size'], config['context_window'], config['d_model']))\n",
    "m = LlamaLayer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        self.linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            (f\"llama_layer_{i}\", LlamaLayer(config)) for i in range(config['n_layers'])\n",
    "        ]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        idx: [batch_size, seq_len]\n",
    "        targets: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        # logits = x # todo\n",
    "        embeds = self.embedding(idx) # [batch_size, seq_len, hidden_size]\n",
    "        logits = self.layers(embeds) # [batch_size, seq_len, vocab_size]\n",
    "        logits = F.softmax(self.linear(logits), dim=-1) # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        \n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_len=20):\n",
    "        \"\"\"\n",
    "        idx: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        for i in range(max_len):\n",
    "            logits = self.forward(idx)\n",
    "            idx = torch.cat([idx, torch.argmax(logits, dim=-1)], dim=-1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741185"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "config = {\n",
    "    \"n_heads\": 8,\n",
    "    \"d_model\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"n_layers\": 4,\n",
    "    \"context_window\": 16,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"epochs\": 1000,\n",
    "    \"log_interval\": 10,\n",
    "}\n",
    "\n",
    "model = Llama(config)\n",
    "sum([p.numel() for p in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 128, 32]' is invalid for input of size 640",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([encode(\u001b[39m\"\u001b[39m\u001b[39mHello\u001b[39m\u001b[39m\"\u001b[39m)])\n\u001b[0;32m----> 2\u001b[0m model(idx)\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[187], line 21\u001b[0m, in \u001b[0;36mLlama.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# logits = x # todo\u001b[39;00m\n\u001b[1;32m     20\u001b[0m embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(idx) \u001b[39m# [batch_size, seq_len, hidden_size]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(embeds) \u001b[39m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m logits \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(logits), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[149], line 22\u001b[0m, in \u001b[0;36mLlamaLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     mid \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprenorm(x))\n\u001b[1;32m     23\u001b[0m     out \u001b[39m=\u001b[39m mid \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(mid)\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[147], line 13\u001b[0m, in \u001b[0;36mMaskedRotaryNultiHeadedAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([head(x) \u001b[39mfor\u001b[39;49;00m head \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(out))\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[147], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(out))\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[146], line 44\u001b[0m, in \u001b[0;36mMaskedRotarySelfAttentionHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mcontext_window\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     43\u001b[0m q_rotated_query_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_q \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mR \u001b[39m# [d x d x m]\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m q \u001b[39m=\u001b[39m q_rotated_query_weight\u001b[39m.\u001b[39mview(m, d, d) \u001b[39m@\u001b[39m x\u001b[39m.\u001b[39;49mview(m, d, b)\n\u001b[1;32m     45\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mview(b, m, d)\n\u001b[1;32m     47\u001b[0m k_rotated_query_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_k \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mR \u001b[39m# [d x d x m]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 128, 32]' is invalid for input of size 640"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor([encode(\"Hello\")])\n",
    "model(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 4.043 | Time 0.087 | ETA in seconds 8.721\n",
      "Epoch 10 | Loss 4.051 | Time 0.887 | ETA in seconds 87.798\n",
      "Epoch 20 | Loss 4.046 | Time 0.770 | ETA in seconds 75.492\n",
      "Epoch 30 | Loss 4.056 | Time 0.820 | ETA in seconds 79.512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[186], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m xs, ys \u001b[39m=\u001b[39m get_batches(dataset, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39mcontext_window\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m _, loss \u001b[39m=\u001b[39m model(xs, targets\u001b[39m=\u001b[39;49mys)\n\u001b[1;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[150], line 21\u001b[0m, in \u001b[0;36mLlama.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# logits = x # todo\u001b[39;00m\n\u001b[1;32m     20\u001b[0m embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(idx) \u001b[39m# [batch_size, seq_len, hidden_size]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(embeds) \u001b[39m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m logits \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(logits), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[149], line 22\u001b[0m, in \u001b[0;36mLlamaLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     mid \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprenorm(x))\n\u001b[1;32m     23\u001b[0m     out \u001b[39m=\u001b[39m mid \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(mid)\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[147], line 13\u001b[0m, in \u001b[0;36mMaskedRotaryNultiHeadedAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([head(x) \u001b[39mfor\u001b[39;49;00m head \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(out))\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[147], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(out))\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[146], line 48\u001b[0m, in \u001b[0;36mMaskedRotarySelfAttentionHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mview(b, m, d)\n\u001b[1;32m     47\u001b[0m k_rotated_query_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_k \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mR \u001b[39m# [d x d x m]\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m k \u001b[39m=\u001b[39m k_rotated_query_weight\u001b[39m.\u001b[39;49mview(m, d, d) \u001b[39m@\u001b[39m x\u001b[39m.\u001b[39mview(m, d, b)\n\u001b[1;32m     49\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(b, m, d)\n\u001b[1;32m     51\u001b[0m B \u001b[39m=\u001b[39m (q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39md_model\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# train\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(.9, .95), weight_decay=.1, eps=1e-9, lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 300, eta_min=1e-5)\n",
    "\n",
    "start_time = time.time()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "    _, loss = model(xs, targets=ys)\n",
    "    loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % config['log_interval'] == 0:\n",
    "        batch_time = time.time() - start_time\n",
    "        losses += [evaluate_loss()]\n",
    "        print(f\"Epoch {epoch} | Loss {loss.item():.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # inspecting to make sure no gradients are exploding\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         if param.grad is not None:\n",
    "        #             print(f\"{name} | grad norm {param.grad.norm().item():.3f} | param norm {param.norm().item():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
